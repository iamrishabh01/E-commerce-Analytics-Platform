{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZg3eWmKAOC8"
      },
      "outputs": [],
      "source": [
        "# ==================================================================\n",
        "# BRONZE TO SILVER PROCESSING\n",
        "# ==================================================================\n",
        "# Initialize Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Ecom_Bronze_to_Silver\") \\\n",
        "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Mount ADLS containers (run once per cluster)\n",
        "def mount_adls(container_name, mount_point):\n",
        "    dbutils.fs.mount(\n",
        "        source = f\"wasbs://{container_name}@adlsretail.blob.core.windows.net\",\n",
        "        mount_point = f\"/mnt/{mount_point}\",\n",
        "        extra_configs = {\"fs.azure.account.key.adlsretail.blob.core.windows.net\": dbutils.secrets.get(\"key-vault\", \"storage-key\")}\n",
        "    )\n",
        "\n",
        "# Mount all containers\n",
        "mount_adls(\"bronze\", \"bronze\")\n",
        "mount_adls(\"silver\", \"silver\")\n",
        "mount_adls(\"gold\", \"gold\")\n",
        "\n",
        "# Read raw data from Bronze layer\n",
        "events_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"/mnt/bronze/events/*.csv\")\n",
        "\n",
        "products_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"/mnt/bronze/products/*.csv\")\n",
        "\n",
        "users_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .load(\"/mnt/bronze/users/*.csv\")\n",
        "\n",
        "# ================ EVENTS TRANSFORMATION ================\n",
        "# Clean and structure events data\n",
        "silver_events = events_df \\\n",
        "    .withColumn(\"event_time\", to_timestamp(col(\"event_time\"))) \\\n",
        "    .withColumn(\"event_date\", to_date(col(\"event_time\"))) \\\n",
        "    .withColumn(\"event_type\",\n",
        "        when(col(\"event_type\") == \"view\", \"product_view\")\n",
        "        .when(col(\"event_type\") == \"cart\", \"cart_add\")\n",
        "        .when(col(\"event_type\") == \"purchase\", \"purchase\")\n",
        "        .otherwise(\"other\")\n",
        "    ) \\\n",
        "    .filter(col(\"event_type\").isin([\"product_view\", \"cart_add\", \"purchase\"])) \\\n",
        "    .dropDuplicates([\"user_id\", \"product_id\", \"event_time\"]) \\\n",
        "    .withColumn(\"price\", col(\"price\").cast(DecimalType(10,2))) \\\n",
        "    .select(\n",
        "        \"event_time\", \"event_date\", \"event_type\",\n",
        "        \"user_id\", \"product_id\", \"category_id\", \"price\"\n",
        "    )\n",
        "\n",
        "# ================ PRODUCTS TRANSFORMATION ================\n",
        "# Clean product catalog\n",
        "silver_products = products_df \\\n",
        "    .withColumnRenamed(\"category_id\", \"category_code\") \\\n",
        "    .withColumn(\"category_name\", split(col(\"category_code\"), \"\\.\").getItem(0)) \\\n",
        "    .withColumn(\"brand\", initcap(col(\"brand\"))) \\\n",
        "    .withColumn(\"price\", col(\"price\").cast(DecimalType(10,2))) \\\n",
        "    .filter(col(\"price\") > 0) \\\n",
        "    .dropDuplicates([\"product_id\"]) \\\n",
        "    .select(\n",
        "        \"product_id\", \"brand\", \"price\",\n",
        "        \"category_code\", \"category_name\"\n",
        "    )\n",
        "\n",
        "# ================ USERS TRANSFORMATION ================\n",
        "# Clean user data\n",
        "silver_users = users_df \\\n",
        "    .withColumn(\"signup_date\", to_date(col(\"signup_date\"))) \\\n",
        "    .withColumn(\"location\",\n",
        "        when(col(\"location\") == \"\", \"unknown\")\n",
        "        .otherwise(col(\"location\"))\n",
        "    ) \\\n",
        "    .withColumn(\"country\", split(col(\"location\"), \",\").getItem(-1)) \\\n",
        "    .dropDuplicates([\"user_id\"]) \\\n",
        "    .select(\"user_id\", \"signup_date\", \"country\", \"location\")\n",
        "\n",
        "# Write to Silver layer (Delta format)\n",
        "silver_events.write.format(\"delta\") \\\n",
        "    .partitionBy(\"event_date\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/mnt/silver/fact_events\")\n",
        "\n",
        "silver_products.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/mnt/silver/dim_products\")\n",
        "\n",
        "silver_users.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"/mnt/silver/dim_users\")\n",
        "\n",
        "# Create Delta tables\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS silver.fact_events\n",
        "    USING DELTA\n",
        "    LOCATION '/mnt/silver/fact_events'\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS silver.dim_products\n",
        "    USING DELTA\n",
        "    LOCATION '/mnt/silver/dim_products'\n",
        "\"\"\")\n",
        "\n",
        "spark.sql(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS silver.dim_users\n",
        "    USING DELTA\n",
        "    LOCATION '/mnt/silver/dim_users'\n",
        "\"\"\")"
      ]
    }
  ]
}